import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns  
import datetime
import gdown
import plotly.express as px
from utils.assets_loader import load_image
from PIL import Image

#Import des biblioth√®ques ML
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error
from prophet import Prophet


def intro():
    # ==========================================================================
    # Initialisation des cl√©s de session_state pour tous les mod√®les et donn√©es
    # ==========================================================================
    if 'df' not in st.session_state:
        st.session_state['df'] = None
        st.session_state['split_date'] = None
        st.session_state['target'] = None
        st.session_state['features'] = None
        st.session_state['rf_metrics_per_region'] = None
        st.session_state['rf_global_mean_metrics'] = None
        st.session_state['xgb_metrics_per_region'] = None
        st.session_state['xgb_global_mean_metrics'] = None

    st.write('## Classification du probl√®me üìÇ')
    st.write("");st.write("") 

    st.markdown(""" 
                <u>Type de probl√®me et t√¢che de machine learning</u> : 

                Pour rappel, notre projet s‚Äôapparente √† de la **pr√©diction de valeurs continues dans une suite temporelle** pr√©sentant plusieurs saisonnalit√©s.
                L'objectif est d'anticiper la demande en √©nergie en fonction du temps, des conditions m√©t√©orologiques et d'autres facteurs exog√®nes. 
                
                Nous avons donc trait√© et fusionn√© l'ensembles des donn√©es expos√©es pr√©c√©demment dans un dataset regroupant nos variables explicatives :   
                """,unsafe_allow_html=True)
    st.write("Echantillon **.sample(10)** : ")
    # --- MODIFICATION ICI ---
    if st.session_state['df'] is not None:
        st.dataframe(st.session_state['df'].sample(5))  # Acc√©der √† df via session_state
    else:
        st.info("Veuillez charger les donn√©es en cliquant sur le bouton 'Charger et Traiter les Donn√©es' dans la section 'Lancement' pour voir un √©chantillon.")
    # --- FIN MODIFICATION ---

    st.markdown(""" Pour simplifier cette restitution, nous allons entra√Æner puis comparer nos mod√®les que sur la **maille horaire**. 
                La robustesse √† long terme sera limit√© √† la fin de la p√©riode de test du jeu de donn√©es. 
                """,unsafe_allow_html=True)
    st.write("---")
    st.write('#### Choix des m√©triques de performance üéØ')
            
    st.markdown("""La m√©trique **MAPE (Mean Absolute Percentage Error)** est notre m√©trique principale car elle est facilement interpr√©table et comparable avec d‚Äôautres mod√®les.
                Nous cherchons d‚Äôune part √† p√©naliser les grandes erreurs compte tenu de l‚Äôenjeu de pr√©diction de consommation au plus juste (**RMSE** faible), 
                tout en pouvant comparer facilement nos diff√©rents mod√®les sur la base de % de variation (MAPE). Enfin, la qualit√© globale du mod√®le doit aussi √™tre √©lev√©e pour tenir compte de mani√®re √©quilibr√©e des sp√©cificit√©s r√©gionales (**Score R2**).""") 
    st.markdown("""
                Pour couvrir l‚Äôensemble des KPI pertinents sur ce probl√®me de r√©gression, nous allons donc r√©cup√©rer chacun des indicateurs type :

                - Erreurs absolues et relatives : **[MAE (Mean Absolute Error)](https://en.wikipedia.org/wiki/Mean_absolute_error)**, **[MAPE](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)**
                - Erreurs quadratiques : **[MSE (Erreur quadratique moyenne)](https://fr.wikipedia.org/wiki/Erreur_quadratique_moyenne)**, **[RMSE (Racine de l'erreur quadratique moyenne)](https://fr.wikipedia.org/wiki/Racine_de_l%27erreur_quadratique_moyenne)**
                - Qualit√© d‚Äôajustement : **[R¬≤ Score (Coefficient de d√©termination)](https://fr.wikipedia.org/wiki/Coefficient_de_d%C3%A9termination)**
                """)
    st.write("---")
    st.write('#### Choix des mod√®les Machine Learning ü§ñ ')    
    st.markdown("""
                De fa√ßon plus limit√©e que le rapport d'√©tude, nous ne pr√©senterons ici que :
 
                - [**Random Forest**](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels), [**XGBoost**](https://en.wikipedia.org/wiki/XGBoost) : deux autres mod√®les, plus g√©n√©ralistes et simples √† entra√Æner.
                - [**Prophet**](https://facebook.github.io/prophet/docs/quick_start.html) : pour challenger notamment la d√©tection des saisonnalit√©s et la robustesse √† long terme.  

                Ces mod√®les sont connus pour bien g√©rer les s√©ries temporelles.
                """)
    st.write('#### Series temporelles (hold-out)‚è≤Ô∏è, encodage, standardisation ? ‚Äù')
    st.markdown("""
                Objectif = √âviter la fuite de donn√©es. Si les donn√©es ne sont pas tri√©es par date et que le train_test_split est al√©atoire, 
                il est possible que des observations tr√®s proches temporellement se r√©partissent entre Train et Test faussant l'entra√Ænement. 
                En triant par date, les donn√©es de test et en ‚Äòsplitant‚Äô sur la fin du jeu de donn√©es, les donn√©es de Test sont vraiment "in√©dites" pour le mod√®le. 
                """)
    st.markdown(""" Avec **Random Forest**, **XGBoost** et **Prophet**, l‚Äôencodage n'apporte pas de b√©n√©fices majeurs par rapport √† une simple variable cat√©gorielle (ex. hour ou dayofweek). 
                De m√™me, la normalisation des donn√©es n‚Äôa pas d‚Äôimpact significatif sur la performance des mod√®les. Nous faisons le choix de laisser les variables sans normalisation et sans transformation variables cycliques.
            """)
    st.write("---")
    st.write('#### Fine tunning - Hyperparam√®tres ')
    st.write("Pour une approche m√©thodique dans la comparaison des 2 mod√®les bas√©s sur des arbres de d√©cisions et travailler avec les meilleurs param√®trages, " \
    "nous avons utilis√© **Grid Search**. Pour all√©ger le besoin de puissance de calcul demand√©s ci-apr√®s, nous laisserons laisserons les param√®tres suivants : "
    "Compromis entre une exigence de m√©moire acceptable pour streamlit et des score √©lev√©s des m√©triques observ√©es.")
    
    code = '''
            current_model = RandomForestRegressor(
                n_estimators=7, # Nombre d'arbres dans la for√™t. Plus il y en a, plus le mod√®le est robuste mais lent.
                max_depth=7, # Profondeur maximale de chaque arbre. Contr√¥le la complexit√© du mod√®le pour √©viter le surapprentissage.
                min_samples_split=2, # Nombre minimum d'√©chantillons requis pour diviser un n≈ìud interne.
                min_samples_leaf=1, # Nombre minimum d'√©chantillons requis pour qu'un n≈ìud soit une feuille.
                random_state=42, # Graine al√©atoire pour la reproductibilit√© des r√©sultats.
                n_jobs=1 # Utile pour Streamlit pour la performance

            current_model = XGBRegressor(
                n_estimators=100,    # Nombre d'estimateurs (arbres)
                max_depth=3,         # Profondeur maximale de l'arbre
                learning_rate=0.05,  # Taux d'apprentissage. R√©duit la contribution de chaque arbre pour rendre le mod√®le plus robuste.
                random_state=42,
                n_jobs=1 
            )
        '''
    st.code(code, language='python')
        ###### image ######
    img = load_image("learning_curve_xgboost.png")
    if img:
            st.image(img, caption="A titre d'exemple, la courbe d'apprentissage XGBoost, et le score RMSE en fonction du nombre d'it√©rations." \
            "Et o√π l'on voit que n_estimator = 100 peut suffire", use_container_width=True)
    else:
            st.warning("‚ùå L‚Äôimage est introuvable dans le dossier `pictures/`.")
        ###### image ######
    st.write("---")
    st.write('## Traitement du dataset ')

def lancement():
    # Bouton pour lancer le traitement des donn√©es et l'affichage
    if st.button("Charger et Traiter les Donn√©es"):
        with st.spinner("Chargement et traitement des donn√©es en cours..."):
            # Assignation directe √† 'df' et stockage sous la cl√© 'df' dans session_state
            df, split_date, target, features = load_process_dataset_modelisation() 
            
            st.session_state['df'] = df
            st.session_state['split_date'] = split_date
            st.session_state['target'] = target
            st.session_state['features'] = features
 
        st.success("Donn√©es charg√©es et pr√©trait√©es avec succ√®s !")
        
        st.subheader("Aper√ßu du DataFrame apr√®s pr√©traitement :")
        st.dataframe(st.session_state['df'].sample(10)) 
        
        st.subheader("Param√®tres de mod√©lisation :")
        st.write(f"**Date de s√©paration (split_date) :** {st.session_state['split_date']}")
        st.write(f"**Variable cible (target) :** `{st.session_state['target']}`")
        st.write(f"**Variables explicatives (features) :**")
        st.write(st.session_state['features'])

        #combined_results_df = pd.concat([
           # st.session_state['rf_metrics_per_region'],
           # st.session_state['xgb_metrics_per_region']
        #], ignore_index=True)

        #st.session_state['combined_results_df'] = combined_results_df
        #st.session_state['features_for_plot'] = st.session_state['features']
    
    # ======================================================================
    # Nouveau bouton pour entra√Æner RF et XGBoost ensemble
    # ======================================================================
    # V√©rifie si les donn√©es sont charg√©es avant d'afficher ce bouton
    if st.session_state['df'] is not None: 
        if st.button("Lancer l'entra√Ænement et l'√©valuation des mod√®les (RF & XGBoost)"):
            # R√©cup√©rer les donn√©es de session_state pour les passer √† la fonction RF_XGB
            df = st.session_state['df'] 
            split_date = st.session_state['split_date']
            target = st.session_state['target']
            features = st.session_state['features']

            # --- Entra√Ænement et √©valuation RandomForest ---
            with st.spinner("Entra√Ænement et √©valuation du mod√®le RandomForest en cours..."):
                # Appel de la fonction renomm√©e RF_XGB pour RandomForest
                rf_metrics_df_per_region, rf_global_mean_metrics = RF_XGB("RandomForest", df, split_date, target, features)
                
                st.session_state['rf_metrics_per_region'] = rf_metrics_df_per_region
                st.session_state['rf_global_mean_metrics'] = rf_global_mean_metrics
            st.success("√âvaluation RandomForest termin√©e !")
            
            st.markdown("---") # S√©parateur visuel entre les mod√®les

            # --- Entra√Ænement et √©valuation XGBoost ---
            with st.spinner("Entra√Ænement et √©valuation du mod√®le XGBoost en cours..."):
                # Appel de la fonction renomm√©e RF_XGB pour XGBoost
                xgb_metrics_df_per_region, xgb_global_mean_metrics = RF_XGB("XGBoost", df, split_date, target, features)

                st.session_state['xgb_metrics_per_region'] = xgb_metrics_df_per_region
                st.session_state['xgb_global_mean_metrics'] = xgb_global_mean_metrics
            st.success("√âvaluation XGBoost termin√©e !")

        # ======================================================================
        # Affichage s√©par√© des r√©sultats RF et XGBoost
        # Ces blocs s'ex√©cutent si les r√©sultats sont pr√©sents dans session_state
        # ======================================================================
        if st.session_state['rf_metrics_per_region'] is not None:
            st.subheader("Performances du mod√®le RandomForest par r√©gion :")
            st.dataframe(st.session_state['rf_metrics_per_region'].set_index('R√©gion').style.highlight_max(axis=0, subset=['R2 Score']).highlight_min(axis=0, subset=['Mean Absolute Error', 'MAPE (%)', 'Root Mean Squared Error', 'Bias']))

            st.subheader("‚ûó Moyennes des m√©triques d'√©valuation RandomForest (Global) :")
            st.dataframe(st.session_state['rf_global_mean_metrics'].to_frame(name='Moyenne').T)

        if st.session_state['xgb_metrics_per_region'] is not None:
            st.markdown("---") 
            st.subheader("Performances du mod√®le XGBoost par r√©gion :")
            st.dataframe(st.session_state['xgb_metrics_per_region'].set_index('R√©gion').style.highlight_max(axis=0, subset=['R2 Score']).highlight_min(axis=0, subset=['Mean Absolute Error', 'MAPE (%)', 'Root Mean Squared Error', 'Bias']))

            st.subheader("‚ûó Moyennes des m√©triques d'√©valuation XGBoost (Global) :")
            st.dataframe(st.session_state['xgb_global_mean_metrics'].to_frame(name='Moyenne').T)

        if (
            st.session_state['rf_metrics_per_region'] is not None
            and st.session_state['xgb_metrics_per_region'] is not None
        ):
            combined_results_df = pd.concat([
                st.session_state['rf_metrics_per_region'],
                st.session_state['xgb_metrics_per_region']
            ], ignore_index=True)
            st.session_state['combined_results_df'] = combined_results_df
            st.session_state['features_for_plot'] = st.session_state['features']
    else:
        st.info("Cliquez sur 'Charger et Traiter les Donn√©es' pour commencer √† visualiser et mod√©liser. " \
        "L'entrainement des mod√®les est ensuite propos√©.")

@st.cache_data    
def load_process_dataset_modelisation():
    #T√©l√©charge et pr√©traite les donn√©es depuis Google Drive."""
    file_id = "1wiXdpj6XHzB1eRxRbvcnsgE21ukVBvXs"  # Ton ID de fichier extrait
    url = f"https://drive.google.com/uc?id={file_id}"  # Lien de t√©l√©chargement direct
    output = "COMPILATION_CONSO_TEMP_POP_2.csv"
    # Non reduced
    # https://drive.google.com/file/d/1wiXdpj6XHzB1eRxRbvcnsgE21ukVBvXs/view?usp=sharing
    #reduce : file_id = "1dunWvb7loR5kWYZwb8BX_lwmYMP0157q" // COMPILATION_CONSO_TEMP_POP_reduced.csv
    
    try:
        gdown.download(url, output, quiet=False)
    except Exception as e:
        st.error(f"Erreur lors du t√©l√©chargement du fichier : {e}")
        st.stop() 

    df= pd.read_csv(output, sep=';',low_memory=False)
    #on_bad_lines="skip", encoding="utf-8"
    
    # Filtrer les donn√©es temporelles pour se concentrer sur une p√©riode pertinente et enlever la Corse
    # Appliquez d'abord les filtres sur le DataFrame original
    df_filtered = df[(df['Date + Heure'] >= '2016-01-01') & 
                     (df['Date + Heure'] <= '2024-12-31') & 
                     (df['R√©gion'] != 'Corse')].copy() # Utilisez .copy() pour √©viter SettingWithCopyWarning

    # Convertir 'Date + Heure' en datetime et la d√©finir comme index pour le DataFrame filtr√©
    df_filtered['Date + Heure'] = pd.to_datetime(df_filtered['Date + Heure'], errors='coerce')
    df_filtered = df_filtered.set_index('Date + Heure')
    df_filtered = df_filtered.sort_index()
    #df_filtered = df_filtered.sort_values(by=[df.index.name, 'R√©gion'])

    # Le DataFrame final √† utiliser sera df_filtered
    df = df_filtered.copy() # On renomme df_filtered en df pour la coh√©rence avec le reste du code


    # Conversion en datetime DATE pour extractions 
    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')
    # Extraire les caract√©ristiques temporelles SUPPLEMENTAIRES √† partir de 'DATE'
    df['year'] = df['Date'].dt.year
    df['month'] = df['Date'].dt.month
    df['day_of_week'] = df['Date'].dt.dayofweek  # Lundi = 0, Dimanche = 6
    df['day_of_year'] = df['Date'].dt.dayofyear
    df['week_of_year'] = df['Date'].dt.isocalendar().week
    #df['PlageHoraire']= df['Heure']
    df['PlageHoraire']= df['Heure'].str[:2].astype(int) # Extraction de l'heure
    df = df.drop(columns=['Date', 'Heure', 'Date - Heure'])
    df = df.sort_index() #NOUVEAU

    # R√©cup√©rer toutes les colonnes du DataFrame
    all_columns = df.columns.tolist()

    # D√©finir la target (√† exclure des features)
    target = 'Consommation (MW)'

    # D√©finir une liste de colonnes √† exclure (en plus de la target)
    exclude_columns = ['R√©gion']

    # S√©lectionner les features en excluant la target et les colonnes √† exclure
    features = [col for col in all_columns if col != target and col not in exclude_columns]

    # D√©finir la proportion de l'ensemble de test
    test_size = 0.20  # Cela signifie 20% des donn√©es pour le test et 80% pour l'entra√Ænement.
    total_observations = len(df)
    # Calculer le nombre d'observations dans l'ensemble d'entra√Ænement (80%)
    train_observations = int(total_observations * (1 - test_size))
    # Calculer le nombre d'observations dans l'ensemble de test (20%)
    test_observations = total_observations - train_observations
    # Calculer la date de s√©paration en se basant sur la 80√®me percentile des observations
    # Le .name r√©cup√®re la valeur de l'index (qui est la date) de cette ligne
    split_date = df.iloc[train_observations - 1].name # Utilisez train_observations - 1 car iloc est 0-index√©

    # Afficher les informations de s√©paration pour justifier les volumes
    print(f"Volume total des entr√©es (observations) : {total_observations}")
    print(f"Proportion d'entra√Ænement d√©sir√©e : {int((1 - test_size) * 100)}%")
    print(f"Nombre d'observations pour l'entra√Ænement : {train_observations} lignes")
    print(f"Proportion de test d√©sir√©e : {int(test_size * 100)}%")
    print(f"Nombre d'observations pour le test : {test_observations} lignes")
    print(f"Date de s√©paration : {split_date}")

    return df, split_date, target, features


def RF_XGB(model_name, df, split_date, target, features):
    """
    Entra√Æne un mod√®le (RandomForest ou XGBoost) pour chaque r√©gion et √©value ses performances.
    Returns:        tuple: DataFrame des m√©triques par r√©gion et Series des m√©triques moyennes globales.
    """
    results = []
    regions = df['R√©gion'].unique()

    # Diviser le DataFrame global en ensembles d'entra√Ænement et de test une seule fois
    train_df = df[df.index < split_date]
    test_df = df[df.index >= split_date]

    for region in regions:
        st.write(f"Entra√Ænement et √©valuation du mod√®le **{model_name}** pour la r√©gion : **{region}**") 
        
        # Filtrer les donn√©es par r√©gion √† partir des ensembles d√©j√† splitt√©s
        train_region_df = train_df[train_df['R√©gion'] == region]
        test_region_df = test_df[test_df['R√©gion'] == region]
            
        X_train = train_region_df[features]
        y_train = train_region_df[target]
        X_test = test_region_df[features]
        y_test = test_region_df[target]

        # =========================================
        # INSTANCIATION DU MOD√àLE + HYPERPARAM√àTRES
        # =========================================
        current_model = None
        if model_name == "RandomForest":
            current_model = RandomForestRegressor(
                n_estimators=7, # Nombre d'arbres dans la for√™t. Plus il y en a, plus le mod√®le est robuste mais lent.
                max_depth=7, # Profondeur maximale de chaque arbre. Contr√¥le la complexit√© du mod√®le pour √©viter le surapprentissage.
                min_samples_split=2, # Nombre minimum d'√©chantillons requis pour diviser un n≈ìud interne.
                min_samples_leaf=1, # Nombre minimum d'√©chantillons requis pour qu'un n≈ìud soit une feuille.
                random_state=42, # Graine al√©atoire pour la reproductibilit√© des r√©sultats.
                n_jobs=1 # Utile pour Streamlit pour la performance
            )
        elif model_name == "XGBoost":
            current_model = XGBRegressor(
                n_estimators=100,    # Nombre d'estimateurs (arbres)
                max_depth=3,         # Profondeur maximale de l'arbre
                learning_rate=0.05,  # Taux d'apprentissage. R√©duit la contribution de chaque arbre pour rendre le mod√®le plus robuste.
                random_state=42,
                n_jobs=1 
            )
        else:
            st.error(f"Mod√®le non support√© pour l'entra√Ænement : {model_name}")
            continue 
        
        current_model.fit(X_train, y_train)

        predictions = current_model.predict(X_test)
        
        # Calculer les m√©triques
        mse = mean_squared_error(y_test, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test, predictions)
        mape = mean_absolute_percentage_error(y_test, predictions) * 100 
        r2 = r2_score(y_test, predictions)
        
        # Moyennes des valeurs r√©elles et pr√©dites
        mean_y_test = np.mean(y_test)
        mean_y_pred = np.mean(predictions) 
        # Calcul du Bias
        bias = mean_y_pred - mean_y_test

        result = {
            'R√©gion': region,
            'Mod√®le': model_name, #AJOUT
            'Moy y_test': mean_y_test,
            'Moy y_pred': mean_y_pred,
            'Bias': bias,
            'Mean Squared Error': mse,
            'Root Mean Squared Error': rmse,
            'Mean Absolute Error': mae,
            'MAPE (%)': mape, 
            'R2 Score': r2
        }
        
        # Ajouter les importances des features si le mod√®le le supporte
        if hasattr(current_model, 'feature_importances_'):
            for feature, importance in zip(X_train.columns, current_model.feature_importances_): 
                result[f'Importance {feature}'] = importance
        
        results.append(result)
        
    results_df = pd.DataFrame(results)
    
    # Calculer la moyenne des m√©triques globales (pour toutes les r√©gions)
    numeric_metrics_cols = ['R2 Score', 'Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'MAPE (%)', 'Bias']
    mean_metrics = results_df[numeric_metrics_cols].mean()

    return results_df, mean_metrics

def plot_feature_importance(combined_results_df, features):

    # Filtrer les colonnes d'importance des features
    importance_cols = [f'Importance {f}' for f in features]
    
    # S√©lectionner les colonnes pertinentes pour le graphique
    df_plot = combined_results_df[['Mod√®le', 'R√©gion'] + importance_cols]
    
    # Faire un melt du DataFrame pour faciliter la visualisation avec Plotly
    # Nous voulons une ligne par (Mod√®le, R√©gion, Feature, Importance)
    df_melted = df_plot.melt(id_vars=['Mod√®le', 'R√©gion'], 
                             value_vars=importance_cols, 
                             var_name='Feature', 
                             value_name='Importance')
    
    # Nettoyer les noms des features (enlever 'Importance ')
    df_melted['Feature'] = df_melted['Feature'].str.replace('Importance ', '')

    # Calculer l'importance moyenne par feature et par mod√®le pour l'affichage global
    avg_importance_df = df_melted.groupby(['Mod√®le', 'Feature'])['Importance'].mean().reset_index()
    
    # Cr√©er le barplot group√©
    fig = px.bar(avg_importance_df, 
                 x='Feature', 
                 y='Importance', 
                 color='Mod√®le', 
                 barmode='group', # Pour grouper les barres par feature et par mod√®le
                 labels={'Feature': 'Feature', 'Importance': 'Importance Moyenne'},
                 height=500)
    
    fig.update_layout(xaxis_title="Features", yaxis_title="Importance Moyenne")
    fig.update_xaxes(categoryorder='total descending') # Ordonner les features par importance totale descendante
    
    return fig

def display_modeling_results_and_plots():

    st.write("---")
    st.subheader(" üß© Importance des Features pour les arbre de d√©cisions")
    if st.button("Afficher l'Importance des Features"):
        if 'combined_results_df' in st.session_state and 'features_for_plot' in st.session_state:
            if st.session_state['combined_results_df'] is not None and st.session_state['features_for_plot'] is not None:
                fig = plot_feature_importance(st.session_state['combined_results_df'], st.session_state['features_for_plot'])
                st.plotly_chart(fig)
            else:
                st.warning("‚ö†Ô∏è Les donn√©es n√©cessaires au calcul des importances ne sont pas disponibles. Veuillez entra√Æner les mod√®les d'abord.")
        else:
            st.info("üí° Cliquez d'abord sur 'Charger et Traiter les Donn√©es' puis 'Entra√Æner les Mod√®les' avant d'afficher l‚Äôimportance des features.")
    st.markdown("---")
    st.header(" ü§ñ Focus Prophet")
        
    st.markdown(""" Il ressort de l'entrainement une capacit√© moindre √† capter la variabilit√© de notre variable cible. T√©moins les m√©triques l√©g√®rement inf√©rieures √† celles de RandomForest et XGBoost :
                 """)
    
    # Cr√©e un dictionnaire avec tes donn√©es
    data = {
        'R√©gion': [
            'Auvergne-Rh√¥ne-Alpes', 'Bretagne', 'Centre-Val de Loire', 'Grand Est',
            'Hauts-de-France', 'Normandie', 'Occitanie', 'Pays de la Loire',
            'Provence-Alpes-C√¥te d\'Azur', '√éle-de-France', 'Nouvelle-Aquitaine',
            'Bourgogne-Franche-Comt√©'
        ],
        'MSE': [618347.408440, 71293.294483, 57860.196918, 273224.467675, 496277.620232, 107251.190415, 408808.501023, 316904.940208, 196585.834993, 780407.570747, 611839.872156, 48594.971564],
        'RMSE': [786.350690, 267.008042, 240.541466, 522.708779, 704.469744, 327.492275, 639.381342, 562.943106, 443.380012, 883.406798, 782.201938, 220.442672],
        'MAE': [628.756138, 207.169993, 192.209835, 426.653469, 571.402178, 237.517623, 512.350058, 424.050043, 334.635441, 708.092105, 602.397156, 173.843995],
        'MAPE': [9.613186, 9.489713, 10.549336, 9.797167, 10.667154, 8.937283, 12.240458, 16.976135, 7.952227, 10.248624, 13.203451, 8.578102],
        'Bias': [-397.169287, 53.035106, -106.187905, -279.899089, -517.764597, -87.048697, -386.316433, -5.547958, -135.732437, -550.626619, -449.839053, -64.854448],
        'R^2': [0.638707, 0.702248, 0.720858, 0.610560, 0.409639, 0.669755, 0.466369, 0.391215, 0.629417, 0.765376, 0.507729, 0.781519]
    }
    df = pd.DataFrame(data)
    df_rounded = df.round(2)
    col1, col2 = st.columns([2, 1]) # La premi√®re colonne est deux fois plus large
    with col1:
        st.write("Voici les m√©triques de performance pour chaque r√©gion :")
        st.dataframe(df_rounded, hide_index=True)
    with col2:
        st.write("Moyenne Globale :")
        # Charge l'image
        try:
            img = load_image("KPI_prophet.png") # Assurez-vous que l'image est dans le m√™me dossier ou sp√©cifiez le chemin complet
            st.image(img, caption="On notera un BIAS moyen n√©gatif qui sous-entend une sous-estimation contrairement aux arbres de d√©cision qui surestiment", use_container_width=True)
        except FileNotFoundError:
            st.warning("‚ùå L‚Äôimage est introuvable. Veuillez v√©rifier le chemin d'acc√®s.")
    st.markdown("""La bonne captation par Prophet des saisonnalit√©s est ici illustr√©e: tendance globale, hebdomadaire, annuelle ou journali√®re.)
                """)
    ###### image ######
    img1 = load_image("saisonnalit√©s1.png")
    img2 = load_image("saisonnalit√©2.png")
    col1, col2 = st.columns(2)
    with col1:
        st.image(img1)
    with col2:
        st.image(img2)
    ###### image ######
    st.markdown("---")
    st.markdown("""    
                """)
    st.markdown("---")

def conclusion():
    ##################
    #INTERPRETATION FEATURES
    st.write("## ‚úÖ Bilan ")
    st.markdown(""" Les mod√®les RandomForest et XGBoost s‚Äôaccordent sur l‚Äôimportance d√©terminante de la temp√©rature moyenne et maximale pour pr√©dire la consommation √©lectrique, 
                refl√©tant l‚Äôimpact du climat sur la demande (chauffage/climatisation). La plage horaire est √©galement cl√©, capturant les variations journali√®res typiques. 
                Les variables calendaires jouent un r√¥le secondaire (XGBoost y est n√©anmoins plus sensible ). Tandis que la population ne variant pas √† cette √©chelle de temps, a peu d‚Äôinfluence 
                """)

    st.markdown(""" 
                Comparatif des mod√®les :

                **XGBoost** surpasse Random Forest avec un R¬≤ sup√©rieur de 0,07, indiquant qu‚Äôil explique significativement mieux la variabilit√© de la consommation √©lectrique.
                Soit une meilleure capacit√© de XGBoost √† capturer les variations fines et les non-lin√©arit√©s. Ses erreurs (RMSE, MAE, MAPE) sont √©galement plus faibles, traduisant des pr√©dictions plus pr√©cises et un biais r√©duit (<u>moins de surestimation</u>.
                A noter : 
                - la diff√©rence de MAPE correspond √† une am√©lioration d‚Äôenviron 10% de la pr√©cision relative au profit de XG Boost
                - XG Boost porte bien son nom en ce qu'il est √©galement plus rapide √† entrainer (sur ce jeu de donn√©es).

                Cependant, Random Forest reste un mod√®le robuste, plus simple √† param√©trer et interpr√©ter, pouvant √™tre pr√©f√©r√© selon les contraintes op√©rationnelles. 
                Le choix d√©pendra donc du compromis entre performance fine et simplicit√© d‚Äôusage.

                """,unsafe_allow_html=True)